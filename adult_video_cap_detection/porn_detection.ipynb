{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0caabbe8aae66f166215b96e0fb51f27c1f826ce7ea0291944d30647cd85c68b4",
   "display_name": "Python 3.7.9 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense,Input,Embedding,Bidirectional,LSTM,Lambda,Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'content/train'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.join('content/','train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adult_video_cap_detection/final_data.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a07465c74229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adult_video_cap_detection/final_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m         )\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    642\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m             )\n\u001b[0;32m    646\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adult_video_cap_detection/final_data.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"adult_video_cap_detection/final_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-07dbbd211c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'titles'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df=df[['titles','label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "\n",
    "import re\n",
    "\n",
    "class CreateDataset():\n",
    "  def __init__(self,df):\n",
    "    self.df=df\n",
    "\n",
    "  def __call__(self):\n",
    "    train_data=[]\n",
    "    values,labels=self.df['titles'].values,self.df['label'].values\n",
    "    for value in values:\n",
    "      x=re.sub(\"[^0-9a-zA-Z]+\",\" \",str(value))\n",
    "      train_data.append(x)\n",
    "\n",
    "    return train_data,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dir(CreateDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=CreateDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data,y_train_data=data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S DAY I SHALL TRY amp STAY POSITIVE BUT I HAVE A LOT ON MY PLATE 2DAY I BARELY EVEN HAVE TIME 2 TWIT NO FAIR ----0\n",
      "Guy with 2 teens ends in ass fuck----1\n",
      "looking for friends im new to this twitter ----0\n",
      "not feeling v good abt myself ----0\n",
      "AroundTheHorn That championship bout was pretty tactical but in a BORING way Leites shoulda came out blazing in the 5th Depressed ----0\n",
      "VID 20161217 WA0086----1\n",
      "Amateur Porn Show 3128646 2 amature video----1\n",
      "bad luck follows me around closer than my own shadow ----0\n",
      "JPBrancati omg I hear ya It s horrid ----0\n",
      "I want to go out and jog right now but my parents would tell me it s too late HOLY CRAP I MISS MY BABY WAAAAAAAAAAAAAAY TOO MUCH ----0\n",
      " y----1\n",
      "tweetdeck isn t working for me i can t keep up with watkins on here ----0\n",
      "He deceived me that he is taking me to his house but ended up fucking in the local bush----1\n",
      "epic amateur cumshot compilation----1\n",
      "Hottie strips bikini----1\n",
      "English----1\n",
      "knee still not ready good luck to my squad pacrim all stars----0\n",
      "jennielees Your blogs RSS feed for Posts seems to be broken in Google Chrome ----0\n",
      "yuvipanda i second that stupid limit ----0\n",
      "Glad to be home Back to our super busy life Feel like I m missing my right arm without AJ 2 more days ----0\n",
      "I have an rash on the right side of my tummy it itched i scratched now it hurts ----0\n",
      "has headache and a tummyache but on the bright side college monday oh yey ----0\n",
      "There is much to be said for drunken singing Shopping time now plans for party under way Doctors apts but must be done SPRING HERE ----0\n",
      "at home without my queen ----0\n",
      "onlysweeter I don t know the dance ----0\n",
      "Slowly dying of boredom Also freezing cold yaaay ----0\n",
      "Arab Woman showing her big boobs----1\n",
      "Miss California wins the award for dumbest answer ever in Miss USA pageant history Also disappointed that The Veronicas lipsynced ----0\n",
      "Ugh im waiting to hear back from a uni ive applied to i really really really wanna get in so much sad times ----0\n",
      "OMG Someone help I can t stop sneeeezing If 1 more person says bless yu I m gonna screeeeam ----0\n",
      "Eek Holiday Spelt wright wrong Why is the English language so confusing I mean write ----0\n",
      "Brincadeira entre amigos----1\n",
      "Sofija That s sucky would you just use vacation time or travel again ----0\n",
      "Student Sucking Big Cock and Deep Swallow----1\n",
      "Sexy milf takes cum deep inside her throat----1\n",
      "carolineroche I am a little bit it just getting up extra early ----0\n",
      "TweetDeck is not working properly for me ----0\n",
      "wish I was in Houston for the Houston Dynamo game instead of here for work ----0\n",
      "I hate SPRINGshowers rainRAIN go away ----0\n",
      "My poor dog s not feeling well She threw up an hour ago and now I think she s having a bad dream She s whining in her sleep ----0\n",
      "Fixed my account picture That Twitter PRO April Fools thing got a lot of people I don t keep track of dates so I didn t realize ----0\n",
      "The young and gorgeous slave in tight rope bondage try to escape but no chance She looks desperate and haggard Poor girl waiting her destiny and her destiny is a hard hand Master ----1\n",
      "lol so how is everyone and btw i have to go to my grans soon ----0\n",
      "hsubidooby hahahah i wanted feeohnah but was taken ----0\n",
      "Best blowjob ever Fucking amazing ----1\n",
      "Just saw the pilot for Caprica It was excellent Have to wait ages for show to start next year ----0\n",
      "says I have a confession to make http plurk com p oz80r----0\n",
      "i feel like i ve been stabbed in the stomach ----0\n",
      "Big Boobs Lucie Wilde Lesbian Playful with BFF----1\n",
      "harriet75 aww sorry your having a bad day what s matter if you don t mind me asking----0\n",
      "Nubiles with large dicks porn----1\n",
      "AppleTV has died dreading diagnosis----0\n",
      "asian Creampie Compilation----1\n",
      "mattg00d i dont like you not having internet You dont tweet as much ----0\n",
      "i m at home while my boyfriend is at the club i wish i was there with him ----0\n",
      "Probably will ----0\n",
      "nan----0\n",
      "spent the entire day with phiroze lt 3 now catching up on homework and studyinggg ----0\n",
      "its pussing ----0\n",
      "BANGBROS Petite Blonde Asian Clara Trinity Gets Dicking In Moving Van Then Is Ditched----1\n",
      "maybe going to the french quarter fest today but it looks like its gonna rain ----0\n",
      "SugarBank I m quasi single not really looking and I have no interests besides pornography I guess we can t make out then ----0\n",
      "CHINITA PROVOCANDO A SU PRIMO----1\n",
      "My busty girlfriend loves sucking and fucking my long cock----1\n",
      "Rockergirl75 Eeek I wanna read now but I can t Later I ll be on there later can t wait ----0\n",
      "My husband gets a nap I don t ----0\n",
      "StDAY I still say you re lucky I wish it was warm here ----0\n",
      "emkins i KNOW ----0\n",
      "Ok so I am going to try to go to bed Have to be up at 8 30 for work ----0\n",
      "EL LONDRES tomorrow having withdrawal symptoms from the besties though Tuesday shall fix that for now music eases academic pain ----0\n",
      "Thinking about going back to work tomorrow ----0\n",
      "A teen getting fuck in the pussy----1\n",
      "Banging of cute teenie----1\n",
      "Basically cried through 90 of Rachel Getting Married ----0\n",
      "Ugh I can t sleep because I m not feeling so great ----0\n",
      "Can t sleep again booerns----0\n",
      "boobs grabbed during fuck tits sucked----1\n",
      "MegSheWrote xxlaurenashley ARIELLEJANVIER Missing my girls in Dallas ----0\n",
      "Laying down got out of work for couple hours NAP TIME Sleepy and sick ----0\n",
      "Only two more days until holidays All my friends are in public schools so we can t hang out on Thursday THEN DISNEYLAND OMG ----0\n",
      "why didn t i have more candy apples in my childhood ----0\n",
      "JazzieBluE JazzieBluE oh my god I am so jelous aghh it was ehhh 6 7hour drive from my house ----0\n",
      "wants to use the 8 directional dance pad pero di compatible sa TV yung game http plurk com p n0hyu----0\n",
      "Little boxes I hope I don t ever live in one ----0\n",
      "brazen hussy I was just telling kiki I wish I could have made it last nightttt ----0\n",
      "Pussy ass lick penetration----1\n",
      "asot400 is down in the trending topics too bad asot400 http bit ly 13i3Yp----0\n",
      "https shon xyz zuULZ----1\n",
      "Just puked ----0\n",
      " Plunge into Chaos Cumshot Madness Porn Music video edited version ----1\n",
      "None of my friends answer texts anymore and my best friend is so busy at work he can t talk ----0\n",
      "Miss u baby r u awake Time to poke ur patients already ----0\n",
      "POV Reverse gangbang as a BD present with Lana Roy Nata Ocean and Liya Silver----1\n",
      "gisellenguyen RS have undergone some changes from what I know They no longer have a deputy ed ----0\n",
      "At Gatwick Watch on BST body 8 hours behind on PDT ----0\n",
      "Am I Going To Jail For This Cutie Used Like Meat ROUGH DOGGYSTYLE MISSIONARY FACIAL SABRINA SPICE BLEACHED RAW----1\n",
      "Rain hope it stops soon----0\n",
      "Amateur Porn amature 940904----1\n",
      "Horny Asian MILF and dauther shared a meaty dick----1\n",
      "I want a Ben hug but I think someone stole his phone coz he s being very bipolar ----0\n",
      "LiziBeeSays I was downtown ALL day yesterday I didnt get to the Garment District tho ----0\n",
      "Slow licking blowjob with oral cumshot Darkfairy8006----1\n",
      "British mature nurse sharing cock in trio----1\n",
      "Cuckolding Compilation----1\n",
      "Anal loving gf getting her booty drilled----1\n",
      "jek91 yess we need to do that too and thats depressing perhaps you should have gone to john hopkins and made things a little more c ----0\n",
      "Slim shaved redhead teen Shinaryen gets banged in POV----1\n",
      "why is it always the fat ones ----0\n",
      "Megan Jones Bust his nuts----1\n",
      "Thinking about practicing my posing routine but worried I m all out of body oil ----0\n",
      "kirstenin Did it involve Furbys ----0\n",
      "Full Night HoneyMoon with Boyfriend----1\n",
      "Derpixon Party Games----1\n",
      "valentiinaa Buu Valen you can t diee I want to hugg you ----0\n",
      "Sofi Argentina Pendeja marplatense putita 15----1\n",
      "djcrystalellis won t let me show the love ----0\n",
      "Man Dani always answers Batty but never me ----0\n",
      "Summer cannot come fast enough Unfortunately I don t think I ve ever had more obstacles separate me from warm weather and relaxation ----0\n",
      "Porn free 1st time----1\n",
      "17 mitzi----1\n",
      "Twaiting for wife to come down stairs then will knock out some TiVo She leaves again tomorrow She has to travel too much ----0\n",
      "DG MMK When would he have rolled me My Twitter was seriously fucked up last night ----0\n",
      "fuck a birthday ----0\n",
      "I m craving food like nobody s business ----0\n",
      "Back and forth Awesome Missing most of the Penguins game Damn ----0\n",
      "watching greek cant believe its the last day ill see you guys june 20 ahh ----0\n",
      "Orall service full of burning excitement----1\n",
      "MKoenigsberg don t be a meanie it doesn t matter the weather out when i could go tanning anytime in a cancer machine ----0\n",
      "carthalis yeah I ve only seen not read Empire of the Sun but I read Crash He was an amazing author ----0\n",
      "Pettite girl show full in crakcam com sex live webcam chat 22----1\n",
      "another lovely day what to do with this one oh i know library ----0\n",
      "Hot Latina amateur pounded on audition----1\n",
      "danecook something came up and i m going to miss your show on May 2nd in Jersey It was my birthday present from my girls ----0\n",
      "j 1st time sex porn----1\n",
      "hate my haircut as usual ----0\n",
      "Lana Rhoades sucking cock----1\n",
      "ill make fresh start i promise xtra sad puppy face ----0\n",
      "back from a mini break in the spanish hillside fantastic but simply too short ----0\n",
      "kidmanproject Shame it won t make it to iPlayer ----0\n",
      "Brunette secretly watching her sister fuck her boyfriend----1\n",
      "MY INTERNET IS SOMEWHAT WORKING I want seb ----0\n",
      "I just woke up and my head is screaming plus my whataburger chkn sandwich was half eaten and I passed out on my couch to sportscenter----0\n",
      "Tomorrow 9am class 11am dentist app root canal arrgh 5pm meeting Between 11am 5pm In pain perhaps ----0\n",
      "Aww Logan got called in ----0\n",
      "Hot Babes Want To Fuck Here OnlySex 6 9 com----1\n",
      "ARGH every time I wash a certain pair of my jeans they become so tight I can t get them off HELP ----0\n",
      "Laura just left and now listening to Elise fill our home with beautiful piano music and song but alas she leaves in a couple hours ----0\n",
      "Hei es Girl von Tinder bl st mir einen extrem geil und schluckt am Ende----1\n",
      "Ass Traffic Caty s tight ass gets licked and fucked----1\n",
      "macintom weird are a couple of sites i can t access to day expatica com being one of them ----0\n",
      "Slut testing her new pussy pump----1\n",
      "considering going to the airport buti dout i can not fair i wanna leave this place for a while but juvi in japan is not the place----0\n",
      "POORN XXX Amateur new girl homemade fuck pt 1----1\n",
      "pressdarling Sorry to hear that ----0\n",
      "My goodness it s freezing down here ----0\n",
      "nick carter Honey what I must do for you tell me quot HI quot quot Hello quot Signs of smoke I will cry ----0\n",
      "Momma Hazel could cook amp mk anythng taste gourmet I miss her ----0\n",
      "mokshjuneja A good movie premier would cheer me up ----0\n",
      "marthagoneMAD agreed I wish it could just be pure and easy fun ----0\n",
      "mother and son sex----1\n",
      "is sitting in computer apps ----0\n",
      "Stud gets to fuck a smoking babe in front of a small audience ----1\n",
      "Teen fucks sweet hot girl----1\n",
      "Ballbusting Surprise with Lady Bellatrix----1\n",
      "I m bloated everywhere ----0\n",
      "JizzOrama Ginger Teen Fucked in Ass and Mouth----1\n",
      "cubedweller Gah not for me on freeview then Still if they brought over the Colbert Report I d consider switching just to get that ----0\n",
      "Trying to get rid of a stupid virus off of our computer Hubby accidentally opened something ridiculous ----0\n",
      "TheGirlOfGlass he s not following me back ----0\n",
      "Cute Skinny Blonde Teen Banged Hard On Couch----1\n",
      "BANGBROS Young Ebony Babe Princess Yummy Fucks Her BBF s Brother----1\n",
      "At Joy 102 5 today It s too nice to be working inside ----0\n",
      "She needs huge dicks ----1\n",
      "Cougar sex party----1\n",
      "I stepped in dog crap when I went for my walk ----0\n",
      "Une futur star du porno se fait prendre le cul----1\n",
      " I love when my master takes control he wants to gangbang me Must see ----1\n",
      "Ok folks the weekend is over Wash this mask off moisturise and then sleep time ----0\n",
      "sostill Should have gone for a cinema voucher We bought him a Lego set and it turns out he already has it ----0\n",
      "JennyMessinger Aww Well if it s any consolation I missed that too I was out with a friend and didn t get back on in time ----0\n",
      "Real pasang 7 hidden cam isteri selingkuh----1\n",
      "KELLY MADISON Sexual Chemistry----1\n",
      "dollarcoin looool that is 300mb of media mebbe not as much as I would like to there will be a lot though just you wait ----0\n",
      "its a mac like WOW Pismo screen cracked Anyone wanna give me one no seriously cant afford to fix it and need it for school lol Mac----0\n",
      "danecook greatt im still trying to get tix idk if im gonna get them though ----0\n",
      "I broke my 8Start I shall have to rebuild my version of this app ----0\n",
      "not going to makke it to boling today got no money ----0\n",
      "webscreamer hahah aww it is scarier than a templates fire Where is my beer you promised ages ago ----0\n",
      "i don t want a friday lab next semester ----0\n",
      "Runaway nubiles porn----1\n",
      "Roxy Dee Ginger Fox Jenna Clarke mini orgy with 3 guys RS172----1\n",
      "charlie horse in the middle of the night terrible dreams sick ----0\n",
      "the stiller der ist leer ----0\n",
      "Threesome With Mum And Stepdaughter----1\n",
      "I need to work out more getting fatter ----0\n",
      "wishes your phone hadn t been turned off http plurk com p oz68d----0\n",
      "MadAudMe Did you delete your facebook ----0\n",
      "iPhone restore is going to take too long Am I truly going to have to go without my phone tonight A terrifying prospect ----0\n",
      "can t find my FB friends on twitter ----0\n",
      "I d my cousin and then had sex----1\n",
      "tenny id ha Its been falling since 5am ----0\n",
      "I came second in 2 games of squares today Always a bridesmaid ----0\n",
      "is back from France ----0\n",
      "janemcmurry too bad i don t have one of those fancy nail dryers I messed them up a little ----0\n",
      "Cabin Fever has kicked in ----0\n",
      "clare 10 I SOOOOO wish I could go to the British Grand Prix ----0\n",
      "pizza night and i feel too sick ----0\n",
      "jasonmitchener well I was doing good right on up to 5 minutes ago whereas I noticed a big tree fell in my backyard ----0\n",
      "Omg pannenkoekenboot Allready seasick while embarking Nice day ahead ----0\n",
      "sweet teen fucks me hard----1\n",
      "i m completely drained i suppose day drinking does that ----0\n",
      "amiefoley Yeah ----0\n",
      "is the worst girlfriend in the universe ----0\n",
      "iamjonathancook thanks now that s going to be in my head all night i m upset you never gave me my other wishes yet ----0\n",
      "off to lunch then some work urggh it is a Sunday ----0\n",
      "L Hutch a rubbish old man pub and was fosters x 23----0\n",
      "Why is it when you have a cold you can t sleep cos your all blocked up ----0\n",
      "Is going to be sad to leave the beautiful barn she has been staying at in Norfolk This weekend has been total bliss ----0\n",
      "Blond Teen College Girl with Big Boobs goes from Cunt to Asshole to Mouth Big Young Tits Perfect Butt Assfucking ATM before she gets her Face Sprayed with Spunk ----1\n",
      "WOULD have rocked the forecast until one last batch of precip snowed out 0 01 quot in the first hour I just can t make up my points FML----0\n",
      "MidnightPR sugar man I miss grits can t get them in Canada ----0\n",
      "R SK dreamcast was so win ----0\n",
      "PCHintsNTips More ways to promote ads and get you to be a consumer of the YT partners by taking away YOUR views that you EARNED away ----0\n",
      "MissCasey22 Bummer ----0\n",
      "just got back from a rather disappointing UFC main event ----0\n",
      "is cleaning house http tinyurl com cezk4p----0\n",
      "fucking a ----0\n",
      "psyche1701 Damn I use Dentyl Guess it s time to find something else Ta for the heads up chuck ----0\n",
      "I donated sperm this afternoon and it really made me think about my own mortality Poor little buggers are probably frozen now ----0\n",
      "Shamsi is very sore with a broken leg http apps facebook com dogbook profile view 4363815----0\n",
      "orangy68 what channel was it on last night i seen the buildup then there was football on Setanta ----0\n",
      "Almost finished my 600 word essay but I don t feel too confident about it ----0\n",
      "Matthew Day Yeah at work ----0\n",
      "cant believe her boyfriend just injured himself in the first game of footy feel like i was going to cry ----0\n",
      "Anal Sex Lovers Aurelly Rebel Wendy Moon in a Hardcore MMF Threesome----1\n",
      "Undressed party porn----1\n",
      "Just called animal control not feeling good about having the kitten picked up but no other options at this point ----0\n",
      "Sorting out high schools for 11yo Nerve wracking stuff Don t want to get it wrong ----0\n",
      "overheardatmoo Wish I could have participated this time ----0\n",
      "is sick and tired glee tomorrow and idk how I feel about that ----0\n",
      "Damnit slept in ----0\n",
      "PALMTREEENT lol I m okay lol but I m mdd cuz I don t get 2 meet yall nxt week when yall come down here ----0\n",
      "Absolutely stunning 19yo guy met on s and fucked----1\n",
      "just had my lunch and did my chores boring ----0\n",
      "Someone has quit following me Well I never said I was interesting ----0\n",
      "theannarose ive never been so sad in my life lets go back and get her ----0\n",
      "tiffslambleg itsonlyamy after about and hr of searching with other cell phone lights and calling i had to give up ----0\n",
      "BLACK4K Bored teen babe is happy to make love to strong black male----1\n",
      "Retired to bed now bit of gta ds before sleeping Hip still poorly ----0\n",
      "JonathanRKnight Awww I soo wish I was there to see you finally comfortable Im sad that I missed it ----0\n",
      "my horny wife----1\n",
      "Petite beauty assfucked by big cocks----1\n",
      "Maddy O Reilly Forever----1\n",
      "Breeding comp of different hotwives----1\n",
      "Receptionist Painal Scene----1\n",
      "stayed at the ees houst caring my my lil one she didnt want me to come home i had to and missed the warly wake up IM msgs from my beau ----0\n",
      "Girl sucks 2 big cocks----1\n",
      "OMG Yankees are getting the A handed to them this sucks so bad ----0\n",
      "arifgan arifgan me too except for the last part ----0\n",
      "bought a dress but might not work cuz it s too long damn my asian short genes ----0\n",
      "chromachris Clean Me ----0\n",
      "Guy fucks mouth of an oriental doxy----1\n",
      "Stuck at a kid s bday paty no hot guys ----0\n",
      "As a kid I chased my cat with a spray bottle LOL randomfacts----0\n",
      " quot I not need quot http twlol com tw v1 133326 lol ichc cat still miss mine dang it ----0\n",
      "hmm shoulder is making some bad noises if i move my arm back and forth i know don t do it fear a return to physio may be required ----0\n",
      "Slow satisfying blow job ----1\n",
      "mmauran ahh i don t know i m not escaping but really i dn t know for me its an undefinable thingie dsn t hv to hv a definition----0\n",
      " Cardinals lost AGAIN ----0\n",
      "nicolerichie I haven t seen Marley amp Me but I m pretty sure it s Marley that passes away ----0\n",
      "One of the best deepthroats ever----1\n",
      "wish my cats would know that on sunday people has the right to sleep late ----0\n",
      "neck herts ----0\n",
      "Got a really bad throat Gonna see if we ve got any yoghurt in ----0\n",
      "says its raining http plurk com p ozw38----0\n",
      "Tissue BABES----1\n",
      "trying to find the driver for my interface so it works on my laptop but my brother isn t online to ask him which version of osx I have ----0\n",
      "has just discovered you shouldn t store vanilla extract in the fridge ----0\n",
      "Milf mom has anal sex with son s buddy----1\n",
      "petewentz OMG i wish i would have been at the venue ive been online waiting for your tweets about the show didnt want to stand in line----0\n",
      "R I P JILL DDD AN DI CAN T THINK OF ANYBODY ELSE WHO I HATE TO MISS AS MUCH AS I HATE MISSING YOU ----0\n",
      "is it friday yet ----0\n",
      "Versione Free Mia madre organizza festini di sesso tra amici e amiche vogliose ----1\n",
      "Best anal ass to mouth ever Pt12----1\n",
      "Brad2TheBone420 He did I watched it last night Whataaaaaaa jerk I m soooo not rooting for him anymore ----0\n",
      "IYAZ08 heyy i cant reply to your DM cuz your not following me xoxo ----0\n",
      "gabysslave thanks you too I have an essay to write ----0\n",
      "life just sucks sometimes ----0\n",
      "Ok battery almost dead Need to stop twittering and watching scores ----0\n",
      "dang i m lazy i ve begun three short stories in the last three weeks and never finished anything will have more focus ----0\n",
      "Adorable blonde amateur sucking and fucking an old guy----1\n",
      "Took niece amp nephew to Blockbuster rented quot Marley amp me quot amp quot Slumdog Millionaire quot Niece kiki799 reinjured ankle last night b ball game ----0\n",
      "MOM MILF s with big breasts getting fucked----1\n",
      "Asian wife fucked by street guy while husband filming----1\n",
      "Upside down honey gives blowjob----1\n",
      "Lisa stupidlamb oh yeah i thought you were talkin about Fightstar cos they are They arnt playin anything good ----0\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(x_train_data,y_train_data):\n",
    "  print(f'{i}----{j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing tf.pad\n",
    "# t = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "# paddings = tf.constant([[1, 1,], [2, 2]])\n",
    "\n",
    "# tf.pad(t, paddings, \"CONSTANT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(x_train_data)\n",
    "sequences=tokenizer.texts_to_sequences(x_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[21, 338, 48, 2, 156, 30, 53, 3, 124],\n",
       " [15563, 424, 2678, 51, 207, 983, 14, 3352, 5098, 690, 3, 3149]]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "sequences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "max([len(i) for i in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence=pad_sequences(sequences,maxlen=60)\n",
    "train_sequences=tf.convert_to_tensor(train_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(43599, 60), dtype=int32, numpy=\n",
       "array([[   0,    0,    0, ...,   53,    3,  124],\n",
       "       [   0,    0,    0, ...,  690,    3, 3149],\n",
       "       [   0,    0,    0, ..., 6627,   16,  428],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  193,  470, 2889],\n",
       "       [   0,    0,    0, ...,  994,  535,  191],\n",
       "       [   0,    0,    0, ..., 5292,  415,   57]])>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label=tf.one_hot(y_train_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(43599, 2), dtype=float32, numpy=\n",
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "y_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class New_Model(tf.keras.Model):\n",
    "    def __init__(self,units=512):\n",
    "        super(New_Model,self).__init__()\n",
    "        self.embedding_layer=tf.keras.layers.Embedding(len(vocab),100)\n",
    "        self.lstm_layer=tf.keras.layers.LSTM(units,return_sequences=True)\n",
    "        self.dense_layer_1=tf.keras.layers.Dense(units/2,activation='relu')\n",
    "        self.dense_layer_2=tf.keras.layers.Dense(2,activation='softmax')\n",
    "        self.dropout_layer=tf.keras.layers.Dropout(0.2)\n",
    "        self.flat_layer=tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x=self.embedding_layer(inputs)\n",
    "        x=self.lstm_layer(x)\n",
    "        x=self.flat_layer(x)\n",
    "        x=self.dropout_layer(x)\n",
    "        x=self.dense_layer_1(x)\n",
    "        x=self.dense_layer_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=New_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = tf.keras.layers.Input(shape=(60,), dtype=tf.float32)\n",
    "initializer=model(x_in, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([43599, 60]), TensorShape([43599, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train_sequences.shape,y_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(60,), dtype=int32, numpy=\n",
       " array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0, 15563,   424,  2678,    51,   207,   983,\n",
       "           14,  3352,  5098,   690,     3,  3149])>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_sequences[1],y_train_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO=tf.data.experimental.AUTOTUNE\n",
    "train_data=tf.data.Dataset.from_tensor_slices((train_sequences[:int(0.8*len(train_sequences))],y_train_label[:int(0.8*len(train_sequences)]))).batch(32,drop_remainder=True).prefetch(AUTO)\n",
    "valid_data=tf.data.Dataset.from_tensor_slices((train_sequences[int(0.8*len(train_sequences)):],y_train_label[int(0.8*len(train_sequences):]))).batch(32,drop_remainder=True).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((32, 60), (32, 2)), types: (tf.int32, tf.float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "train_data,valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "filepath='porn_detection.h5'\n",
    "model_checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=filepath,monitor='loss', save_best_only=True,verbose=1,save_weights_only=True,mode='auto',period=1)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])  # we may use SGD or mini-gradient-descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1362/1362 [==============================] - 901s 661ms/step - loss: 0.0717 - accuracy: 0.9744\n",
      "Epoch 2/5\n",
      "1362/1362 [==============================] - 1009s 741ms/step - loss: 0.0147 - accuracy: 0.9956\n",
      "Epoch 3/5\n",
      "1362/1362 [==============================] - 671s 493ms/step - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 4/5\n",
      "1362/1362 [==============================] - 673s 494ms/step - loss: 0.0052 - accuracy: 0.9985\n",
      "Epoch 5/5\n",
      "1362/1362 [==============================] - 661s 485ms/step - loss: 0.0045 - accuracy: 0.9989\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c66cfd4088>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "model.fit(data,steps_per_epoch=int(len(train_sequences)/32),validation_data=valid_data,epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"new__model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 60, 100)           3975000   \n_________________________________________________________________\nlstm (LSTM)                  (None, 60, 512)           1255424   \n_________________________________________________________________\ndense (Dense)                (None, 256)               7864576   \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 514       \n_________________________________________________________________\ndropout (Dropout)            (None, 30720)             0         \n_________________________________________________________________\nflatten (Flatten)            (None, 30720)             0         \n=================================================================\nTotal params: 13,095,514\nTrainable params: 13,095,514\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data"
   ]
  }
 ]
}